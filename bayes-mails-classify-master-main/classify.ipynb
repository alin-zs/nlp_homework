{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87905b06-cf71-4926-92de-edeea5a2de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from jieba import cut\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de54803-2e2e-49bf-b739-aada8b4f3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(filename):\n",
    "    \"\"\"读取文本并过滤无效字符和长度为1的词\"\"\"\n",
    "    words = []\n",
    "    with open(filename, 'r', encoding='utf-8') as fr:\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            # 过滤无效字符\n",
    "            line = re.sub(r'[.【】0-9、——。，！~\\*]', '', line)\n",
    "            # 使用jieba.cut()方法对文本切词处理\n",
    "            line = cut(line)\n",
    "            # 过滤长度为1的词\n",
    "            line = filter(lambda word: len(word) > 1, line)\n",
    "            words.extend(line)\n",
    "    return words\n",
    "\n",
    "all_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02c63b5-1a1d-44e8-8153-0e88fc77cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(top_num):\n",
    "    \"\"\"遍历邮件建立词库后返回出现次数最多的词\"\"\"\n",
    "    filename_list = ['邮件_files/{}.txt'.format(i) for i in range(151)]\n",
    "    # 遍历邮件建立词库\n",
    "    for filename in filename_list:\n",
    "        all_words.append(get_words(filename))\n",
    "    # itertools.chain()把all_words内的所有列表组合成一个列表\n",
    "    # collections.Counter()统计词个数\n",
    "    freq = Counter(chain(*all_words))\n",
    "    return [i[0] for i in freq.most_common(top_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d69fd21-b04f-40d5-88f7-fddd2c0f64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(feature_type='high_frequency'):\n",
    "    top_words = get_top_words(100)\n",
    "    if feature_type == 'high_frequency':\n",
    "        vector = []\n",
    "        for words in all_words:\n",
    "            word_map = list(map(lambda word: words.count(word), top_words))\n",
    "            vector.append(word_map)\n",
    "        vector = np.array(vector)\n",
    "    elif feature_type == 'tfidf':\n",
    "        texts = []\n",
    "        for words in all_words:\n",
    "            texts.append(' '.join(words))\n",
    "        vectorizer = TfidfVectorizer(vocabulary=top_words)\n",
    "        vector = vectorizer.fit_transform(texts).toarray()\n",
    "    else:\n",
    "        raise ValueError(\"feature_type 必须是 'high_frequency' 或 'tfidf'\")\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77508688-9bdb-4787-b66b-b293e97e838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(feature_type='high_frequency'):\n",
    "    vector = extract_features(feature_type)\n",
    "    # 0 - 126.txt为垃圾邮件标记为1；127 - 151.txt为普通邮件标记为0\n",
    "    labels = np.array([1] * 127 + [0] * 24)\n",
    "\n",
    "    # 使用SMOTE进行过采样\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(vector, labels)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    return model, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02643e6-92a0-4cc5-9c81-a66bc13e1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filename, model, top_words, feature_type='high_frequency'):\n",
    "    \"\"\"对未知邮件分类\"\"\"\n",
    "    # 构建未知邮件的词向量\n",
    "    words = get_words(filename)\n",
    "    if feature_type == 'high_frequency':\n",
    "        current_vector = np.array(\n",
    "            tuple(map(lambda word: words.count(word), top_words)))\n",
    "    elif feature_type == 'tfidf':\n",
    "        text = ' '.join(words)\n",
    "        vectorizer = TfidfVectorizer(vocabulary=top_words)\n",
    "        current_vector = vectorizer.fit_transform([text]).toarray().flatten()\n",
    "    # 预测结果\n",
    "    result = model.predict(current_vector.reshape(1, -1))\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3351057b-9db3-4951-9c3e-0f0820914213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, top_words, feature_type='high_frequency'):\n",
    "    vector = extract_features(feature_type)\n",
    "    labels = np.array([1] * 127 + [0] * 24)\n",
    "    predictions = []\n",
    "    for i in range(151):\n",
    "        filename = f'邮件_files/{i}.txt'\n",
    "        pred = predict(filename, model, top_words, feature_type)\n",
    "        predictions.append(pred)\n",
    "    report = classification_report(labels, predictions)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5808ce6-6f7a-4979-a080-bd0a5f802dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用高频词特征预测：\n",
      "151.txt分类情况: 垃圾邮件\n",
      "152.txt分类情况: 垃圾邮件\n",
      "153.txt分类情况: 普通邮件\n",
      "154.txt分类情况: 垃圾邮件\n",
      "155.txt分类情况: 普通邮件\n",
      "高频词特征模型评估报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75        24\n",
      "           1       1.00      0.87      0.93       127\n",
      "\n",
      "    accuracy                           0.89       151\n",
      "   macro avg       0.80      0.94      0.84       151\n",
      "weighted avg       0.94      0.89      0.90       151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [453, 151]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_high_frequency)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 使用TF - IDF加权特征训练模型\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_tfidf, top_words_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfidf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m使用TF - IDF加权特征预测：\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m151\u001b[39m, \u001b[38;5;241m156\u001b[39m):\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(feature_type)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 使用SMOTE进行过采样\u001b[39;00m\n\u001b[0;32m      7\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_resampled, y_resampled)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\imblearn\\base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py:480\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    474\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`BaseEstimator._validate_data` is deprecated in 1.6 and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 1.7. Use `sklearn.utils.validation.validate_data` instead. This \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction becomes public and is part of the scikit-learn developer API.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_data(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py:1389\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1389\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [453, 151]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 使用高频词特征训练模型\n",
    "    model_high_frequency, top_words_high_frequency = train_model(feature_type='high_frequency')\n",
    "    print('使用高频词特征预测：')\n",
    "    for i in range(151, 156):\n",
    "        result = predict(f'邮件_files/{i}.txt', model_high_frequency, top_words_high_frequency, feature_type=\"high_frequency\")\n",
    "        print(f'{i}.txt分类情况: {\"垃圾邮件\" if result == 1 else \"普通邮件\"}')\n",
    "    report_high_frequency = evaluate_model(model_high_frequency, top_words_high_frequency, feature_type='high_frequency')\n",
    "    print(\"高频词特征模型评估报告：\")\n",
    "    print(report_high_frequency)\n",
    "\n",
    "    # 使用TF - IDF加权特征训练模型\n",
    "    model_tfidf, top_words_tfidf = train_model(feature_type='tfidf')\n",
    "    print('使用TF - IDF加权特征预测：')\n",
    "    for i in range(151, 156):\n",
    "        result = predict(f'邮件_files/{i}.txt', model_tfidf, top_words_tfidf, feature_type=\"tfidf\")\n",
    "        print(f'{i}.txt分类情况: {\"垃圾邮件\" if result == 1 else \"普通邮件\"}')\n",
    "    report_tfidf = evaluate_model(model_tfidf, top_words_tfidf, feature_type='tfidf')\n",
    "    print(\"TF - IDF加权特征模型评估报告：\")\n",
    "    print(report_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b49e7c-a4f8-4707-9f33-52ef34a596ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
