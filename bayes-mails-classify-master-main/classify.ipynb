{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87905b06-cf71-4926-92de-edeea5a2de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from jieba import cut\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de54803-2e2e-49bf-b739-aada8b4f3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(filename):\n",
    "    \"\"\"读取文本并过滤无效字符和长度为1的词\"\"\"\n",
    "    words = []\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = line.strip()\n",
    "                # 过滤无效字符\n",
    "                line = re.sub(r'[.【】0-9、——。，！~\\*]', '', line)\n",
    "                # 使用 jieba.cut() 方法对文本切词处理\n",
    "                line = cut(line)\n",
    "                # 过滤长度为1的词\n",
    "                line = filter(lambda word: len(word) > 1, line)\n",
    "                words.extend(line)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件 {filename} 未找到。\")\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02c63b5-1a1d-44e8-8153-0e88fc77cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(top_num, filename_list):\n",
    "    \"\"\"遍历邮件建立词库后返回出现次数最多的词\"\"\"\n",
    "    all_words = []\n",
    "    for filename in filename_list:\n",
    "        all_words.extend(get_words(filename))\n",
    "    # collections.Counter() 统计词个数\n",
    "    freq = Counter(all_words)\n",
    "    return [i[0] for i in freq.most_common(top_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d69fd21-b04f-40d5-88f7-fddd2c0f64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filename_list, feature_method='top_words', top_num=100):\n",
    "    if feature_method == 'top_words':\n",
    "        top_words = get_top_words(top_num, filename_list)\n",
    "        vector = []\n",
    "        for filename in filename_list:\n",
    "            words = get_words(filename)\n",
    "            word_map = list(map(lambda word: words.count(word), top_words))\n",
    "            vector.append(word_map)\n",
    "        return np.array(vector), top_words\n",
    "    elif feature_method == 'tfidf':\n",
    "        corpus = []\n",
    "        for filename in filename_list:\n",
    "            words = get_words(filename)\n",
    "            corpus.append(\" \".join(words))\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vector = vectorizer.fit_transform(corpus)\n",
    "        return vector.toarray(), vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77508688-9bdb-4787-b66b-b293e97e838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(filename_list, feature_method='top_words', top_num=100):\n",
    "    vector, top_words = extract_features(filename_list, feature_method, top_num)\n",
    "    # 0 - 126.txt 为垃圾邮件标记为 1；127 - 151.txt 为普通邮件标记为 0\n",
    "    labels = np.array([1] * 127 + [0] * 24)\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vector, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 样本平衡处理\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # 模型评估\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"使用 {feature_method} 特征的模型评估报告：\")\n",
    "    print(report)\n",
    "\n",
    "    return model, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02643e6-92a0-4cc5-9c81-a66bc13e1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filename, model, top_words, feature_method='top_words'):\n",
    "    \"\"\"对未知邮件分类\"\"\"\n",
    "    if feature_method == 'top_words':\n",
    "        # 构建未知邮件的词向量\n",
    "        words = get_words(filename)\n",
    "        current_vector = np.array(\n",
    "            tuple(map(lambda word: words.count(word), top_words)))\n",
    "    elif feature_method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(vocabulary=top_words)\n",
    "        words = get_words(filename)\n",
    "        corpus = [\" \".join(words)]\n",
    "        current_vector = vectorizer.fit_transform(corpus).toarray()[0]\n",
    "\n",
    "    # 预测结果\n",
    "    result = model.predict(current_vector.reshape(1, -1))\n",
    "    return '垃圾邮件' if result == 1 else '普通邮件'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3351057b-9db3-4951-9c3e-0f0820914213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\15113\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.766 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 top_words 特征的模型评估报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       1.00      0.92      0.96        25\n",
      "\n",
      "    accuracy                           0.94        31\n",
      "   macro avg       0.88      0.96      0.91        31\n",
      "weighted avg       0.95      0.94      0.94        31\n",
      "\n",
      "使用高频词特征进行预测：\n",
      "151.txt 分类情况:普通邮件\n",
      "152.txt 分类情况:垃圾邮件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 tfidf 特征的模型评估报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60         6\n",
      "           1       0.89      0.96      0.92        25\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.82      0.73      0.76        31\n",
      "weighted avg       0.86      0.87      0.86        31\n",
      "\n",
      "\n",
      "使用 TF-IDF 特征进行预测：\n",
      "151.txt 分类情况:垃圾邮件\n",
      "152.txt 分类情况:垃圾邮件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 系统找不到指定的文件。\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"D:\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"D:\\Anaconda3\\envs\\nlp_course\\lib\\subprocess.py\", line 505, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"D:\\Anaconda3\\envs\\nlp_course\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"D:\\Anaconda3\\envs\\nlp_course\\lib\\subprocess.py\", line 1436, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filename_list = ['邮件_files/{}.txt'.format(i) for i in range(151)]\n",
    "\n",
    "    # 选择使用高频词特征训练模型\n",
    "    model_top_words, top_words_top_words = train_model(filename_list, feature_method='top_words', top_num=100)\n",
    "    print(\"使用高频词特征进行预测：\")\n",
    "    print('151.txt 分类情况:{}'.format(predict('邮件_files/151.txt', model_top_words, top_words_top_words,\n",
    "                                               feature_method='top_words')))\n",
    "    print('152.txt 分类情况:{}'.format(predict('邮件_files/152.txt', model_top_words, top_words_top_words,\n",
    "                                               feature_method='top_words')))\n",
    "\n",
    "    # 选择使用 TF-IDF 特征训练模型\n",
    "    model_tfidf, top_words_tfidf = train_model(filename_list, feature_method='tfidf')\n",
    "    print(\"\\n使用 TF-IDF 特征进行预测：\")\n",
    "    print('151.txt 分类情况:{}'.format(predict('邮件_files/151.txt', model_tfidf, top_words_tfidf,\n",
    "                                               feature_method='tfidf')))\n",
    "    print('152.txt 分类情况:{}'.format(predict('邮件_files/152.txt', model_tfidf, top_words_tfidf,\n",
    "                                               feature_method='tfidf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5808ce6-6f7a-4979-a080-bd0a5f802dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
